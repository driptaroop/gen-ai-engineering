{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# One and Few Shot Prompting\n",
    "When creating prompts for AI models, it is helpful to provide examples. These examples can help the model understand what you are asking for. Examples are especially useful when you want to steer the model to a certain output structure or pattern.\n",
    "A one-shot prompt, provides a single example, hence the name one-shot. The idea is the model has an example it can imitate to best complete the task. A few-shot prompt provides multiple examples to the model. This approach shows the model a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples of the desired pattern increases the chance the model follows the pattern.\n",
    "\n",
    "The number of examples you need for few-shot prompting depends on a few factors, including the complexity of the task, the quality of the examples, and the capabilities of the generative AI (gen AI) model you are using. As a general rule of thumb, you should use at least three to five examples for few-shot prompting. However, you may need to use more examples for more complex tasks, or you may need to use fewer due to the input length limitation of your model.\n",
    "\n",
    "## Example 1"
   ],
   "id": "6e6480ab05971d05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:09:19.721369Z",
     "start_time": "2025-04-23T15:08:57.638144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "question: how many days are there in a week?\n",
    "answer: 7\n",
    "question: what is the capital of france?\n",
    "answer: paris\n",
    "question: what is the tallest mountain in the world?\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "34472dba6f9019f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mount everest"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Example 2",
   "id": "9e819efaf3b3b914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:10:03.861042Z",
     "start_time": "2025-04-23T15:09:58.906277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "5753c67181a153b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was so excited to see the fireworks, she started to farduddle with glee!"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Example 3",
   "id": "2c10b04dcfed7fb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:11:41.500844Z",
     "start_time": "2025-04-23T15:11:32.430195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "Parse a customer's pizza order into valid JSON:\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella?\n",
    "\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]\n",
    "}\n",
    "```\n",
    "\n",
    "Now, I would like a large pizza, with the first half cheese and\n",
    "mozzarella. And the other tomato sauce, ham and pineapple.\n",
    "JSON Response:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "29199865a087eac9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"type\": \"halved\",\n",
      "  \"ingredients\": [\n",
      "    [\"cheese\", \"mozzarella\"],\n",
      "    [\"tomato sauce\", \"ham\", \"pineapple\"]\n",
      "  ]\n",
      "}\n",
      "```"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations\n",
    "Zero shot, one shot, and few shot prompting are not perfect. One of the examples where it doesn't work correctly is reasoning tasks. In these cases, the model may not be able to understand the reasoning process and may not be able to provide a correct answer. This is because the model is not trained on reasoning tasks and does not have the necessary knowledge to perform them."
   ],
   "id": "b267448465ed4a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:16:05.927409Z",
     "start_time": "2025-04-23T15:15:58.842280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "The prime numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "The prime numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "The prime numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "64ebc9d41b8f8530",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prime numbers in this group are 5, 7.\n",
      "Their sum is 5 + 7 = 12, which is an even number.\n",
      "So the answer is True.\n",
      "\n",
      "A: The answer is True."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is wrong because 13 is also a prime number.",
   "id": "4a1f46ad461a7543"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
