{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# General Prompting\n",
    "\n",
    "Generally, prompts are often created in one of three formats,\n",
    "1. Question\n",
    "2. Incomplete statement\n",
    "3. Instruction\n",
    "\n",
    "## 1. Question\n",
    "Question prompts are the most common type of prompt. They are used to ask the LLM a question and expect an answer. The question can be open-ended or closed-ended, depending on the desired output."
   ],
   "id": "20f8b0742143838d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:16:01.955892Z",
     "start_time": "2025-04-23T14:15:48.236345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"what is the tallest mountain in the world?\",\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "cf486fc9cdf8db66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest mountain in the world is **Mount Everest**.\n",
      "\n",
      "Here's a breakdown of its height and location:\n",
      "\n",
      "*   **Height:** 8,848.86 meters (29,031.7 feet) - This is the officially recognized height, determined by a joint measurement between Nepal and China.\n",
      "*   **Location:** The Himalayas, on the border between Nepal and Tibet (China).\n",
      "\n",
      "\n",
      "\n",
      "While Mount Everest is the tallest above sea level, it's important to note that **Mauna Kea** in Hawaii is taller when measured from its base on the ocean floor, but most of it is underwater."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Incomplete statement\n",
    "Incomplete statement prompts are used to provide the LLM with a partial statement and expect it to complete the statement. This type of prompt is often used to generate text that is similar to a given text."
   ],
   "id": "9df4b068b5ccba6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:18:57.765673Z",
     "start_time": "2025-04-23T14:18:45.747007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"the tallest mountain in the world is\",\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "ec38574df9b58bbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest mountain in the world is **Mount Everest**.\n",
      "\n",
      "Here's a bit more detail:\n",
      "\n",
      "*   **Height:** 8,848.86 meters (29,031.7 feet)\n",
      "*   **Location:** Himalayas, on the border between Nepal and Tibet (China)\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to know more about Mount Everest!"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Instruction\n",
    "Instruction prompts are used to give the LLM a specific instruction and expect it to follow the instruction. This type of prompt is often used to generate text that is similar to a given text."
   ],
   "id": "415467ebc2aefe9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:20:05.244839Z",
     "start_time": "2025-04-23T14:19:57.310779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"write the name of the tallest mountain in the world\",\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "fa08fa49b3303d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest mountain in the world is **Mount Everest**.\n",
      "\n",
      "\n",
      "\n",
      "It stands at 8,848.86 meters (29,031.7 feet) above sea level."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Combinations\n",
    "These types of prompts are not mutually exclusive. You can combine them to create more complex prompts. For example, you can ask a question or an instruction and provide an incomplete statement as a hint. This can help the LLM generate more accurate and relevant responses."
   ],
   "id": "f68c8a7f46d6225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:44:49.450257Z",
     "start_time": "2025-04-23T14:44:22.963047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: I think the summer vacation was great.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "5465d20f841e59e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **Positive**\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "The word \"great\" is a clearly positive descriptor. The sentence expresses a positive feeling about the summer vacation."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this example, we mixed question and incomplete statement prompts.",
   "id": "481320dac1370c16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional Prompt types: System, Contextual and Role-based prompting\n",
    "\n",
    "- **System Prompting**: System prompting sets the overall context and purpose for the language model. It defines the ‚Äòbig picture‚Äô of what the model should be doing, like translating a language, classifying a review etc. It defines the model‚Äôs fundamental capabilities and overarching purpose.\n",
    "- **Contextual prompting**: Contextual prompting provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what‚Äôs being asked and tailor the response accordingly. It provides immediate, task-specific information to guide the response. It‚Äôs highly specific to the current task or input, which is dynamic.\n",
    "- **Role-based prompting**: Role-based prompting assigns a specific role or persona to the model. It tells the model how to behave or respond based on a particular character or perspective. This can be useful for generating responses that are more aligned with a specific tone, style, or viewpoint. It‚Äôs about defining the model‚Äôs behavior and perspective in the conversation. It frames the model‚Äôs output style and voice. It adds a layer of specificity and personality.\n",
    "\n",
    "Distinguishing between system, contextual, and role prompts provides a framework for designing prompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type influences the language model‚Äôs output.\n",
    "\n",
    "### Example"
   ],
   "id": "46bbe9d81a2bdf20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:42:02.866865Z",
     "start_time": "2025-04-23T21:42:00.884896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "Review: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    system=system_prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "d5cfbed7b9f5f6a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Can also add the system prompt to the prompt itself. This is a common practice in LLM prompting, where the system prompt is included in the main prompt to provide context and guidance for the model's response.",
   "id": "73f34e531316a725"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:48:13.730273Z",
     "start_time": "2025-04-23T21:48:06.311759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "Instruction: write a short paragraph about the provided location and places to visit nearby. Write only the paragraph.\n",
    "Role: you are a travel guide.\n",
    "Context: You are writing a twitter post about a travel destination.\n",
    "Location: Edinburgh, Scotland\n",
    "post:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "910b28e6d0c2df28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∞‚ú® Edinburgh, Scotland! Explore the historic Edinburgh Castle, wander the charming Royal Mile, and hike up Arthur's Seat for stunning city views. Beyond the capital, Loch Ness & Inverness are a scenic day trip away, or delve into the Scottish Borders for rolling hills & beautiful abbeys. History, nature, & whisky ‚Äì Edinburgh has it all! #Scotland #Edinburgh #Travel #VisitScotland \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this example, we used a combination of system, contextual, and role-based prompting. The system prompt sets the overall context for the task, the contextual prompt provides specific details about the location, and the role-based prompt assigns a specific role to the model as a travel guide.",
   "id": "1c0cf84acbaef238"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
