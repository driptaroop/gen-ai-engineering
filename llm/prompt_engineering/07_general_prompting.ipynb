{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# General Prompting\n",
    "\n",
    "Generally, prompts are often created in one of three formats,\n",
    "1. Question\n",
    "2. Incomplete statement\n",
    "3. Instruction\n",
    "\n",
    "## 1. Question\n",
    "Question prompts are the most common type of prompt. They are used to ask the LLM a question and expect an answer. The question can be open-ended or closed-ended, depending on the desired output."
   ],
   "id": "20f8b0742143838d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:16:01.955892Z",
     "start_time": "2025-04-23T14:15:48.236345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"what is the tallest mountain in the world?\",\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "cf486fc9cdf8db66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest mountain in the world is **Mount Everest**.\n",
      "\n",
      "Here's a breakdown of its height and location:\n",
      "\n",
      "*   **Height:** 8,848.86 meters (29,031.7 feet) - This is the officially recognized height, determined by a joint measurement between Nepal and China.\n",
      "*   **Location:** The Himalayas, on the border between Nepal and Tibet (China).\n",
      "\n",
      "\n",
      "\n",
      "While Mount Everest is the tallest above sea level, it's important to note that **Mauna Kea** in Hawaii is taller when measured from its base on the ocean floor, but most of it is underwater."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Incomplete statement\n",
    "Incomplete statement prompts are used to provide the LLM with a partial statement and expect it to complete the statement. This type of prompt is often used to generate text that is similar to a given text."
   ],
   "id": "9df4b068b5ccba6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:18:57.765673Z",
     "start_time": "2025-04-23T14:18:45.747007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"the tallest mountain in the world is\",\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "ec38574df9b58bbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest mountain in the world is **Mount Everest**.\n",
      "\n",
      "Here's a bit more detail:\n",
      "\n",
      "*   **Height:** 8,848.86 meters (29,031.7 feet)\n",
      "*   **Location:** Himalayas, on the border between Nepal and Tibet (China)\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to know more about Mount Everest!"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Instruction\n",
    "Instruction prompts are used to give the LLM a specific instruction and expect it to follow the instruction. This type of prompt is often used to generate text that is similar to a given text."
   ],
   "id": "415467ebc2aefe9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:20:05.244839Z",
     "start_time": "2025-04-23T14:19:57.310779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"write the name of the tallest mountain in the world\",\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "fa08fa49b3303d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest mountain in the world is **Mount Everest**.\n",
      "\n",
      "\n",
      "\n",
      "It stands at 8,848.86 meters (29,031.7 feet) above sea level."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Combinations\n",
    "These types of prompts are not mutually exclusive. You can combine them to create more complex prompts. For example, you can ask a question or an instruction and provide an incomplete statement as a hint. This can help the LLM generate more accurate and relevant responses."
   ],
   "id": "f68c8a7f46d6225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:37:09.811890Z",
     "start_time": "2025-04-23T14:37:06.496214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import generate, GenerateResponse\n",
    "from typing import Iterator\n",
    "\n",
    "prompt = \"\"\"\n",
    "question: how many days are there in a week?\n",
    "answer: 7\n",
    "question: what is the capital of france?\n",
    "answer: paris\n",
    "question: what is the tallest mountain in the world?\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=prompt,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ],
   "id": "3a8562993137bd46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mount everest"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this example, we mixed question and incomplete statement prompts. The LLM is given a list of questions and answers, and it is expected to complete the last question with the correct answer (few short prompts). The LLM uses the context of the previous questions and answers to generate the correct response.",
   "id": "481320dac1370c16"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
