{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98bef666ab52c85",
   "metadata": {},
   "source": [
    "# Temparature\n",
    "Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is deterministic: the highest probability token is always selected (though note that if two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0).\n",
    "\n",
    "Temperatures close to the max tend to create more random output. And as temperature gets higher and higher, all tokens become equally likely to be the next predicted token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62cb9051e7ea390",
   "metadata": {},
   "source": [
    "## How It Works (Conceptually)\n",
    "1. The model generates a probability distribution over all possible next tokens (say 50,000+).\n",
    "2. These probabilities are logits (raw scores) which are then softmaxed into a probability distribution.\n",
    "3. Temperature is applied before softmax like this:\n",
    "\n",
    "```python\n",
    "adjusted_logits = logits / temperature\n",
    "probs = softmax(adjusted_logits)\n",
    "```\n",
    "So, the temperature directly scales the logits before they go through softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb6b69f19ca14e",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d387256600b1a071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:38:34.685408Z",
     "start_time": "2025-04-23T12:38:34.452737Z"
    }
   },
   "outputs": [],
   "source": [
    "from ollama import generate, Options, GenerateResponse\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d534c691806ad43",
   "metadata": {},
   "source": [
    "### Temparature = 0\n",
    "\n",
    "|             |                                                                                       |\n",
    "|-------------|---------------------------------------------------------------------------------------|\n",
    "| Name        | 01_temp_0                                                                             |\n",
    "| Model       | Gemma3:12b                                                                            |\n",
    "| Temperature | 0                                                                                     |\n",
    "| Token Limit | -                                                                                     |\n",
    "| Top-K       | -                                                                                     |\n",
    "| Top-P       | -                                                                                     |\n",
    "| Prompt      | what is the capital of germany?                                                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2208551626058d90",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-23T12:38:34.702440Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is **Berlin**.\n",
      "\n",
      "\n",
      "\n",
      "It's also the largest city in Germany!"
     ]
    }
   ],
   "source": [
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"what is the capital of Germany?\",\n",
    "    options=Options(temperature=0),\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "     print(chunk.response, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec0f4255f10dca",
   "metadata": {},
   "source": [
    "This response does not change with different runs. The model always returns the same output. This is because the temperature is set to 0, which means that the model will always select the token with the highest probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1bc9f67bd9298",
   "metadata": {},
   "source": [
    "### Temparature = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0abe06b6ccfa0",
   "metadata": {},
   "source": [
    "|             |                                                                                                  |\n",
    "|-------------|--------------------------------------------------------------------------------------------------|\n",
    "| Name        | 01_temp_1_1                                                                                      |\n",
    "| Model       | Gemma3:12b                                                                                       |\n",
    "| Temperature | 1                                                                                                |\n",
    "| Token Limit | -                                                                                                |\n",
    "| Top-K       | -                                                                                                |\n",
    "| Top-P       | -                                                                                                |\n",
    "| Prompt      | what is the capital of germany?                                                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d9bef3945ed4d",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "502251f33418a8db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:29:07.393490Z",
     "start_time": "2025-04-23T12:29:04.534861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is **Berlin**.\n",
      "\n",
      "\n",
      "\n",
      "It's a vibrant and historic city!"
     ]
    }
   ],
   "source": [
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"what is the capital of Germany?\",\n",
    "    options=Options(temperature=1),\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666729f5e218a3f",
   "metadata": {},
   "source": [
    "#### Example 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542d38a6b6d8df94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:29:57.109655Z",
     "start_time": "2025-04-23T12:29:53.810774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is **Berlin**.\n",
      "\n",
      "\n",
      "\n",
      "It's also the largest city in Germany!"
     ]
    }
   ],
   "source": [
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"what is the capital of Germany?\",\n",
    "    options=Options(temperature=1),\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508b5712d3f5265",
   "metadata": {},
   "source": [
    "This response changes with different runs. The model returns different outputs each time. This is because the temperature is set to 1, which means that the model will select the next token based on a probability distribution. The higher the temperature, the more random the output will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4eb7913ac2b8ea",
   "metadata": {},
   "source": [
    "### Temparature = 10 (unhinged mode)\n",
    "|             |                                 |\n",
    "|-------------|---------------------------------|\n",
    "| Name        | 01_temp_10_1                    |\n",
    "| Model       | Gemma3:12b                      |\n",
    "| Temperature | 10                              |\n",
    "| Token Limit | -                               |\n",
    "| Top-K       | -                               |\n",
    "| Top-P       | -                               |\n",
    "| Prompt      | what is the capital of germany? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8896de179564cd",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca08c5438e787f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:33:04.902308Z",
     "start_time": "2025-04-23T12:32:53.306636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The **federal⌁ **German Empire – The \"federal country’£of Switzerland. Berlinis\n",
      "\n",
      "berlind\n",
      "sist the ꩜onpiredal!t the isd lrgssissta isci, with capital in of in in, the nation ѕрr. iis also one s, capitalss and h⸴ssincc a nation e is of capitalst on, from r as r\n",
      "l, federal ooo of eess!s!t d as he as from with h to of oo h ss yess st\n",
      "Germanyss st hhe ess!!it y ess ooo .of it is a great and it st tt to r oo ee!h e ss"
     ]
    }
   ],
   "source": [
    "stream: Iterator[GenerateResponse] = generate(\n",
    "    model=\"gemma3:12b\",\n",
    "    prompt=\"what is the capital of Germany?\",\n",
    "    options=Options(temperature=10),\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.response, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707e09da34e7133",
   "metadata": {},
   "source": [
    "This meaningless word salads are the result of setting the temperature to 10. The model is now in \"unhinged mode\", where it generates random and nonsensical text. This is because the temperature is set to a very high value, which means that the model will select the next token based on a very flat probability distribution.\n",
    "which means that all tokens are equally likely to be selected. This results in a completely random output that does not make any sense.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
