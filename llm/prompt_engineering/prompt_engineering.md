# Prompt Engineering

## What is prompt engineering?
Prompt engineering is the process of designing and refining prompts to elicit the desired response from a language model. 
It involves understanding how the model interprets different types of input and crafting prompts that guide the model towards generating relevant and accurate outputs.

## Tuning LLMs with sampling controls
Tuning prompts is one of the ways to improve the usefulness of LLMs responses. But it is not the only way.
Another way is to tune the model response via sampling controls. LLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the
next token could be, with each token in the LLM’s vocabulary getting a probability. Those token probabilities are then sampled to determine what the next produced token will be.

There are several parameters that can be adjusted to influence the sampling process and the resulting output.

- **Temperature**: Controls the randomness of the model's output. A lower temperature (e.g., 0.2) makes the output more deterministic, while a higher temperature (e.g., 0.8) increases randomness and creativity.
    See [Temperature](./01_temparature.ipynb) for more details and examples.
- **Top-P (nucleus sampling)**: sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM’s vocabulary).
    See [Top-P](./02_top_p.ipynb) for more details and examples.
- **Top-K**: sampling selects the top K most likely tokens from the model’s predicted distribution. The higher top-K, the more creative and varied the model’s output; the lower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to greedy decoding.
    See [Top-K](./03_top_k.ipynb) for more details and examples.
- **Output length**: The maximum number of tokens to generate in the output. This can be set to a specific value or left to the model to determine based on the input prompt. Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically
  or textually succinct in the output it creates, it just causes the LLM to stop predicting more tokens once the limit is reached. If your needs require a short output length, you’ll also possibly need to engineer your prompt to accommodate.
   See [Output length](./04_output_length.ipynb) for more details and examples.
- **Stop sequences**: A stop sequence is a string of text that, when generated by the model, will cause it to stop generating further tokens. Specifying stop sequences is another way to control the length and structure of the model's response.
    See [Stop sequences](./05_stop_sequences.ipynb) for more details and examples.
- **Repetition penalty**: A repetition penalty is a value that can be applied to the model's output to discourage it from repeating the same tokens or phrases. This can help to improve the diversity and creativity of the generated text.
    See [Repetition penalty](./06_repetition_penalty.ipynb) for more details and examples.

### Sampling controls: Putting it all together
If temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet both the top-K and top-P criteria are candidates for the next predicted token. Then
temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If only top-K or top-P is available, the behavior is the same but only the one top-K or P setting is used.

If temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then randomly selected from to produce a single next predicted token.
At extreme settings of one sampling configuration value, that one sampling setting either cancels out other configuration settings or becomes irrelevant.

### Sampling controls: Recommendations
- **Temperature**: 0.2 to 0.5 for deterministic outputs, 0.7 to 1.0 for more creative outputs.
- **Top-P**: 0.9 to 0.95 for a balance between creativity and coherence. 0.99 for more creative outputs.
- **Top-K**: 25 to 40 for a balance between creativity and coherence. ~100 for more creative outputs.
- If your task always has a single correct answer (e.g., answering a math or coding problem), start with a temperature of 0.

## Prompting
Prompting is the process of providing input to a language model in a way that guides its response. Compared to sampling controls, prompting is a more direct way to influence the model's output.

### General Prompting Basics
Generally, prompts are often created in one of three formats,
1. Question
2. Incomplete statement
3. Instruction

Check out the [Prompting](./07_general_prompting.ipynb) notebook for more details and examples.

### Prompting Techniques
There are several techniques that can be used to improve the effectiveness of prompts. For example,
- [**Zero-shot prompting**](./08_zero_shot_prompting.ipynb)
- [**One and few-shot prompting**](./09_one_few_shot_prompting.ipynb)
- [**Step back prompting**](./10_step_back_prompting.ipynb)
- [**Chain of thought prompting**](./11_chain_of_thought_prompting.ipynb)
- [**Self-consistency prompting**](./12_self_consistency_prompting.ipynb)
- [**Tree of thought prompting**](./13_tree_of_thought_prompting.ipynb)

